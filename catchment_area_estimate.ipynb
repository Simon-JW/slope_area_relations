{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     44
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#from jupyterthemes import jtplot\n",
    "#jtplot.style()\n",
    "import numpy as np\n",
    "import operator\n",
    "import seaborn as sns; sns.set()\n",
    "from landlab import FieldError\n",
    "from landlab.utils import get_watershed_mask\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import gdal\n",
    "from scipy import ndimage\n",
    "from scipy.ndimage.filters import *\n",
    "import os\n",
    "import math\n",
    "from osgeo import osr\n",
    "from fractions import Fraction\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "# import plotting tools\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import matplotlib as mpl\n",
    "from pylab import show, figure\n",
    "from landlab.plot.imshow import imshow_grid \n",
    "\n",
    "# import necessary landlab components\n",
    "from landlab import RasterModelGrid, HexModelGrid\n",
    "from landlab.components import FlowAccumulator, LakeMapperBarnes, SinkFillerBarnes\n",
    "from landlab.components import(FlowDirectorD8, \n",
    "                               FlowDirectorDINF, \n",
    "                               FlowDirectorMFD, \n",
    "                               FlowDirectorSteepest)\n",
    "from landlab.components import DepressionFinderAndRouter\n",
    "# import landlab plotting functionality\n",
    "from landlab.plot.drainage_plot import drainage_plot\n",
    "from pylab import show, figure\n",
    "\n",
    "# create a plotting routine to make a 3d plot of our surface. \n",
    "def surf_plot(mg, surface='topographic__elevation', \n",
    "              title='Surface plot of topography', colormap = cm.gray):\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "\n",
    "    # Plot the surface.\n",
    "    Z = (mg.at_node[surface].reshape(mg.shape))#[y1:y2, x1:x2]\n",
    "    color = colormap((Z-Z.min())/(Z.max()-Z.min()))\n",
    "    surf = ax.plot_surface(mg.node_x.reshape(mg.shape),#[y1:y2, x1:x2]\n",
    "                           mg.node_y.reshape(mg.shape),#[y1:y2, x1:x2]\n",
    "                           Z,\n",
    "                           rstride=1, cstride=1,\n",
    "                           facecolors=color,\n",
    "                           linewidth=0.,\n",
    "                           antialiased=False)\n",
    "    ax.view_init(elev=35, azim=-120)\n",
    "    ax.set_xlabel('X axis')\n",
    "    ax.set_ylabel('Y axis')\n",
    "    ax.set_zlabel('Elevation')\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0,
     6,
     18,
     42,
     60,
     73,
     86,
     92,
     103,
     112,
     120
    ]
   },
   "outputs": [],
   "source": [
    "def rectangleWindow(m, n):\n",
    "    \"\"\"Takes a value for number of rows (m) and number of columns (n) such that\n",
    "       m and n are both positive real numbers and creates a rectangle of \n",
    "       boolian 'True' values.\"\"\"\n",
    "    rectangle = np.ones((m, n), dtype=bool) \n",
    "    return rectangle\n",
    "def number_of_values(Window):\n",
    "    \"\"\"This funciton takes the shape function as an input and returns a number \n",
    "        of values present in the specified shape. \n",
    "        \n",
    "        This can be different for a different window shape and to initialize\n",
    "        requires the specification of the function for the given window type and \n",
    "        parameter values required for that input function.\n",
    "        \n",
    "        To initialize this function for shape == rectangle type 'number_of_values(rectangleWindow(m,n)) \n",
    "        where m and n are any positive real number as per the rectangleWindow function.\"\"\"\n",
    "    denominator = sum(sum(Window > 0))\n",
    "    return denominator\n",
    "def slopeWindow(DEM_slope, x_cellsize, y_cellsize):\n",
    "    \"\"\"This function implements slope calculation using the same algorithm\n",
    "       as ARCGIS (Outlined on the page 'How Slope Works').\n",
    "       This particular example of the function is written such that it\n",
    "       will only work if called within the ndimage generic_filter (as the first input).\n",
    "       This is because the index arguments for a-e are given for the 1d array created\n",
    "       by the generic_filter function after extracting values from the 3,3 rectangle window.\n",
    "       NOTE: THIS FUNCTION ONLY WORKS WITH A 3x3 RECTANGLE WINDOW.\"\"\"\n",
    "    a = DEM_slope[0]; b = DEM_slope[1]; c = DEM_slope[2]\n",
    "    d = DEM_slope[3]; e = DEM_slope[4]; f = DEM_slope[5]\n",
    "    g = DEM_slope[6]; h = DEM_slope[7]; i = DEM_slope[8]\n",
    "    dzdx = ((c + (2*f) + i) - (a + (2*d) + g)) / (8 * x_cellsize)\n",
    "    dzdy = ((g + (2*h) + i) - (a + (2*b) + c)) / (8 * y_cellsize)\n",
    "    rise_run = np.sqrt(dzdx**2 + dzdy**2)\n",
    "    slope_degrees = np.arctan(rise_run) * (180/math.pi)\n",
    "    slope_percent = rise_run * 100\n",
    "    #Can also ask it to return slope_degrees but askinh for both causes it to throw and error.\n",
    "    return slope_percent\n",
    "\n",
    "# The 'extra_arguments' variable requires a value that represents r in the PCTL function defined above.\n",
    "# The reason it need to be assigned to a seperate variable is that the generic_filter function only allows the\n",
    "# input function (PCTL in this case) to take one argument (S1). Then, if the input function normally \n",
    "# takes more than one argument the 'extra_arguments' variable needs to be defined as a tuple \n",
    "# (hence (3,) instead of (3)).\n",
    "def slopeWindowDegrees(DEM, x_cellsize, y_cellsize):\n",
    "    \"\"\"This function implements slope calculation using the same algorithm\n",
    "       as ARCGIS (Outlined on the page 'How Slope Works').\n",
    "       This particular example of the function is written such that it\n",
    "       will only work if called within the ndimage generic_filter (as the first input).\n",
    "       This is because the index arguments for a-e are given for the 1d array created\n",
    "       by the generic_filter function after extracting values from the 3,3 rectangle window.\n",
    "       NOTE: THIS FUNCTION ONLY WORKS WITH A 3x3 RECTANGLE WINDOW.\"\"\"\n",
    "    a = DEM[0]; b = DEM[1]; c = DEM[2]\n",
    "    d = DEM[3]; e = DEM[4]; f = DEM[5]\n",
    "    g = DEM[6]; h = DEM[7]; i = DEM[8]\n",
    "    dzdx = ((c + (2*f) + i) - (a + (2*d) + g)) / (8 * x_cellsize)\n",
    "    dzdy = ((g + (2*h) + i) - (a + (2*b) + c)) / (8 * y_cellsize)\n",
    "    rise_run = np.sqrt(dzdx**2 + dzdy**2)\n",
    "    slope_degrees = np.arctan(rise_run) * (180/math.pi)\n",
    "    slope_percent = rise_run * 100\n",
    "    #Can also ask it to return slope_degrees but asking for both causes it to throw and error.\n",
    "    return slope_degrees\n",
    "def planCurvature(DEM, cellsize):\n",
    "    \"\"\"This process is taken from Change (2014, Introduction to Geographic Information\n",
    "    systems, Page 284).\"\"\"\n",
    "    Z1 = DEM[0]; Z2 = DEM[1]; Z3 = DEM[2]\n",
    "    Z4 = DEM[3]; Z0 = DEM[4]; Z5 = DEM[5]\n",
    "    Z6 = DEM[6]; Z7 = DEM[7]; Z8 = DEM[8]\n",
    "    D = (((Z4 + Z5)/2) - Z0) / cellsize**2\n",
    "    E = (((Z2 + Z7)/2) - Z0) / cellsize**2\n",
    "    F = (Z3 - Z1 + Z6 - Z8)/ (4 * cellsize**2)\n",
    "    G = (Z5 - Z4) / (2 * cellsize)\n",
    "    H = (Z2 - Z7) / (2 * cellsize)\n",
    "    plan_curvature = (2 * (D*(H**2) + E*(G**2) - (F*G*H))) / (G**2 + H**2)\n",
    "    return plan_curvature\n",
    "def profileCurvature(DEM, cellsize):\n",
    "    \"\"\"This process is taken from Change (2014, Introduction to Geographic Information\n",
    "       systems, Page 284).\"\"\"\n",
    "    Z1 = DEM[0]; Z2 = DEM[1]; Z3 = DEM[2]\n",
    "    Z4 = DEM[3]; Z0 = DEM[4]; Z5 = DEM[5]\n",
    "    Z6 = DEM[6]; Z7 = DEM[7]; Z8 = DEM[8]\n",
    "    D = (((Z4 + Z5)/2) - Z0) / cellsize**2\n",
    "    E = (((Z2 + Z7)/2) - Z0) / cellsize**2\n",
    "    F = (Z3 - Z1 + Z6 - Z8)/ (4 * cellsize**2)\n",
    "    G = (Z5 - Z4) / (2 * cellsize)\n",
    "    H = (Z2 - Z7) / (2 * cellsize)\n",
    "    profile_curvature = (-2 * (D*(G**2) + E*(H**2) + (F*G*H))) / (G**2 + H**2)\n",
    "    return profile_curvature\n",
    "def circleWindow(radius):\n",
    "    \"\"\"Takes a value for radius (r where r is any positive real number) and creates \n",
    "       a circular window using that radius.\"\"\"\n",
    "    y, x = np.ogrid[-radius: radius + 1, -radius: radius + 1]\n",
    "    circle = x**2 + y**2 <= radius**2\n",
    "    return circle\n",
    "def find_median_value(Window):\n",
    "    \"\"\"This function takes the shape function and returns the median value \n",
    "        for all valid values (values that fall in the circle) arranged into a \n",
    "        1d array. The function also takes the number_of_values function as an input.\n",
    "        To execute this function type 'find_median_value(shape(r)) where r is any integer.\n",
    "        #Note: using median like this only gives the correct value for circles with odd \n",
    "        radius values.\"\"\"\n",
    "    no_values = number_of_values(Window)\n",
    "    value_range = np.arange(0, no_values + 1)\n",
    "    central_value = int(np.median(value_range))\n",
    "    return central_value\n",
    "def difference_from_mean_elevation(elev):\n",
    "    \"\"\"This function only works as an inside function to generic_filter function below. This\n",
    "       is because generic_filter will take a 2d array and reshape it into a 1d array. Without this \n",
    "       step the 'central_value' variable will be outside of the array dimensions. \n",
    "       x = input DEM and r = radius of search window \"\"\"\n",
    "    centroid = elev[central_value]                        \n",
    "    mean = np.nanmean(elev)#Count number of values greater than centroid value\n",
    "    diff = centroid - mean\n",
    "    return diff\n",
    "def PCTL(x):\n",
    "    \"\"\"This function only works as an inside function to generic_filter function below. This\n",
    "       is because generic_filter will take a 2d array and reshape it into a 1d array. Without this \n",
    "       step the 'central_value' variable will be outside of the array dimensions. \n",
    "       x = input DEM and r = radius of search window \"\"\"\n",
    "    centroid = x[central_value]                        \n",
    "    y = np.sum(x < centroid)/num_values#Count number of values greater than centroid value\n",
    "    return y\n",
    "def catchmentDicts(area_grid, gn, gh_nodes):\n",
    "    \"\"\"Creates a dictionary of node ID:catcment area values.\"\"\"\n",
    "    area_dict = {}\n",
    "    keys = gh_nodes\n",
    "    for i in keys:\n",
    "        area_grid_copy = np.copy(area_grid)\n",
    "        corresponding_grid_cell = np.isin(gn, i).astype(bool)\n",
    "        area_grid_copy[~corresponding_grid_cell] = np.nan\n",
    "        area = np.unique(area_grid_copy[~np.isnan(area_grid_copy)])[0]\n",
    "        area_dict[i] = area\n",
    "    return area_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_DEM_resolution = 1\n",
    "location = (r'C:\\PhD\\manuscript\\flow_routing_algorithms\\data')\n",
    "os.chdir(location)\n",
    "input_geotiff = gdal.Open('fann_2013_1m.tif')\n",
    "ig = np.array(input_geotiff.GetRasterBand(1).ReadAsArray())\n",
    "#input_DEM = scipy.ndimage.zoom(x, input_DEM_resolution / initial_resolution, order = 1)\n",
    "input_DEM = ig\n",
    "gully_head_layer = gdal.Open('core_nodes_1m.tif')\n",
    "gh = np.array(gully_head_layer.GetRasterBand(1).ReadAsArray()).astype('uint8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################\n",
    "flow_acc_surf = np.copy(input_DEM).astype('float64');\n",
    "#################################################################################################\n",
    "rows = flow_acc_surf.shape[0];\n",
    "cols = flow_acc_surf.shape[1];\n",
    "mg = RasterModelGrid((rows,cols), 1);\n",
    "z1 = mg.add_field('topographic__elevation', flow_acc_surf, at = 'node');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_nodes = np.array(mg.nodes.reshape(mg.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(grid_nodes, cmap=\"terrain\");\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolate all gully head nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_nodes = np.multiply(gh, grid_nodes);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create array of only gully head nodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh_array = np.unique(find_nodes);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn gully head node array into a list and remove the value 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh_list = gh_array.tolist();\n",
    "gh_list.remove(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gh_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work through list finding index positions of each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catchment_area_dict = {}\n",
    "catchment_area_node_list = []\n",
    "distance_from_node = 10\n",
    "contributing_fraction = 0.9\n",
    "\n",
    "for i in gh_list:\n",
    "    print(i)\n",
    "    current_node = i\n",
    "    result = np.where(grid_nodes == current_node)\n",
    "    #################################################################################################\n",
    "    # Find the index position (in the whole DEM) that corresponds to the current gully head point in the list.\n",
    "    row_index = result[0][0];\n",
    "    col_index = result[1][0];\n",
    "    #################################################################################################\n",
    "    # Create a subset based on above. Plus 1 to bottom abnd right so that the currenmmt gully ead node is always the \n",
    "    # centraL grid node of the subset.\n",
    "    subset_row_top = int(row_index - distance_from_node)\n",
    "    subset_row_bot = int(row_index + distance_from_node) + 1\n",
    "    subset_col_left = int(col_index - distance_from_node)\n",
    "    subset_col_right = int(col_index + distance_from_node) + 1\n",
    "    #################################################################################################\n",
    "    # Restrict the size of the subset in cases where it is on the edge of a DEM.\n",
    "    subset_row_top_adjusted = max(subset_row_top, 0);\n",
    "    subset_row_bot_adjusted = min(subset_row_bot, input_DEM.shape[0]);\n",
    "    subset_col_left_adjusted = max(subset_col_left, 0);\n",
    "    subset_col_right_adjusted = min(subset_col_right, input_DEM.shape[1]);\n",
    "    #################################################################################################\n",
    "    # Find the offset required by restricting the size of the subset around DEM edges.\n",
    "    # This offset is needed to find the new location of the current gully head grid cell (no longer center of the subset).\n",
    "    offset_row_top = int(np.sqrt((subset_row_top - subset_row_top_adjusted)**2)) * -1;\n",
    "    offset_row_bot = int(np.sqrt((subset_row_bot - subset_row_bot_adjusted)**2));\n",
    "    offset_col_left = int(np.sqrt((subset_col_left - subset_col_left_adjusted)**2)) * -1;\n",
    "    offset_col_right = int(np.sqrt((subset_col_right - subset_col_right_adjusted)**2));\n",
    "    #################################################################################################\n",
    "    # Create subset around the current gully head grid node. \n",
    "    dem_subset_copy = np.copy(input_DEM);\n",
    "    dem_subset = dem_subset_copy[subset_row_top_adjusted:subset_row_bot_adjusted, \n",
    "                                 subset_col_left_adjusted:subset_col_right_adjusted]\n",
    "    #################################################################################################\n",
    "    # Find the latitude and longitude index positions of the gully head node in the subset DEM.\n",
    "    node_lat = distance_from_node + offset_row_top\n",
    "    node_lon = distance_from_node + offset_col_left\n",
    "    #################################################################################################  \n",
    "    # Run M8 for the subset DEM.\n",
    "    flow_acc_surf_sub = np.copy(dem_subset).astype('float64');\n",
    "    \n",
    "    rows_sub = flow_acc_surf_sub.shape[0];\n",
    "    cols_sub = flow_acc_surf_sub.shape[1];\n",
    "    mg1 = RasterModelGrid((rows_sub,cols_sub), 1);\n",
    "    z1 = mg1.add_field('topographic__elevation', flow_acc_surf_sub, at = 'node');\n",
    "    \n",
    "    sfb = SinkFillerBarnes(mg1, method = 'Steepest', ignore_overfill = True);\n",
    "    sfb.run_one_step();\n",
    "    fa = FlowAccumulator(mg1,\n",
    "                        surface = 'topographic__elevation',\n",
    "                        flow_director = 'FlowDirectorMFD',\n",
    "                        diagonals = True);\n",
    "    \n",
    "    fa.run_one_step()\n",
    "    fd = FlowDirectorMFD(mg1, 'topographic__elevation', diagonals = True);\n",
    "    fd.run_one_step()\n",
    "    #################################################################################################\n",
    "    # Extract required grid properties\n",
    "    da = np.array(mg1.at_node['drainage_area'].round(4)); # Drainage area.\n",
    "    frn = mg1.at_node['flow__receiver_node'];# Flow receiver nodes.\n",
    "    drainage_area = da.reshape(mg1.shape);\n",
    "    frp = np.array(mg1.at_node['flow__receiver_proportions']);# Flow receiver proportions\n",
    "    flow_rec_surf_rows = cols_sub * rows_sub;\n",
    "    flow_rec_surf = frp.reshape(flow_rec_surf_rows,8);\n",
    "    all_receiver_proportions = np.copy(frp);\n",
    "    grid_nodes_sub = np.array(mg1.nodes.reshape(mg1.shape));\n",
    "    #################################################################################################\n",
    "    central_node = grid_nodes_sub[node_lat, node_lon]\n",
    "    # Specific the size of the area to be checked. This represents a physical distance from the \n",
    "    # outlet but has no unit of measure. Its maximum size is the size of the 'distance_from_node' variable.\n",
    "    \n",
    "    # This value is used to initialise the search.\n",
    "    catchment_outlet_id = central_node\n",
    "    \n",
    "    # Initate an outer loop that iterates through every distance interval from the outlet. Each iteration checks whether \n",
    "    # an area equal to the 'contributing_fraction' variable below is being directed in only one direction (converging).\n",
    "    # Otherwise it moves to the nect grid cell downslope in the direction of steepest slope.\n",
    "    \n",
    "    # Create a grid of node value sfor the subset DEM.\n",
    "    node_location = mg1.nodes.reshape(da.shape);\n",
    "    for j in range(0, distance_from_node):\n",
    "        # Begin at the point (node) digitized as the intersection between the central flow line and the gully head.\n",
    "        if j == 0:\n",
    "            candidate_gridcell = catchment_outlet_id;\n",
    "        else:\n",
    "            candidate_gridcell = next_cell_downstream;\n",
    "        # Get all receiver nodes of the current node.\n",
    "        grid_cell_receiver_nodes = frp[candidate_gridcell,:];\n",
    "        # Check if there is a node that receiving >= 'contributing_fraction' from the current node.\n",
    "        if np.amax(grid_cell_receiver_nodes) < contributing_fraction:\n",
    "            # Check if this is the last possible grid cell in the subset that could meet the criteria. If so, then there\n",
    "            # is no suitable grid cell (node).\n",
    "            if j == distance_from_node - 1:\n",
    "                catchment_area_dict[i] = 'No grid cell found'\n",
    "            else:\n",
    "                # Otherwise, find the next downslope grid cell receiving the highest proportion of flow and check that \n",
    "                # in the next iteration.\n",
    "                index_next_cell_downstream = np.where(grid_cell_receiver_nodes == np.amax(grid_cell_receiver_nodes))[0][0];\n",
    "                next_cell_downstream = frn[candidate_gridcell, index_next_cell_downstream];\n",
    "        else:\n",
    "            # Find the position of the gridcell (node) that meets the criteria both in the subset and in the whole DEM\n",
    "            # and record the node ID.\n",
    "            final_node_index_lat =  np.where(grid_nodes_sub == candidate_gridcell)[0][0]\n",
    "            final_node_index_lon =  np.where(grid_nodes_sub == candidate_gridcell)[1][0]\n",
    "            # Find the offest between the index position of the node the loop ends on, and the index position of\n",
    "            # the node that it started on. \n",
    "            final_lat_offset = final_node_index_lat - node_lat\n",
    "            final_lon_offset = final_node_index_lon - node_lon\n",
    "            # Then apply this offest to the index position of the starting node with respect top the whole DEM to find the \n",
    "            # node ID of the grid cell that met the criteria above.\n",
    "            final_row_index = row_index + final_lat_offset\n",
    "            final_col_index = col_index + final_lon_offset\n",
    "            node_of_area_record = grid_nodes[final_row_index, final_col_index]\n",
    "            \n",
    "            catchment_area_dict[i] = node_of_area_record\n",
    "            catchment_area_node_list.append(node_of_area_record)\n",
    "            break\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(gh_list), len(catchment_area_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catchment_area_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(catchment_area_node_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlet_ids = np.isin(grid_nodes,catchment_area_node_list).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlet_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_geotiff = gdal.Open('fann_D4_5m.tif')\n",
    "D4 = np.array(input_geotiff.GetRasterBand(1).ReadAsArray())\n",
    "\n",
    "input_geotiff = gdal.Open('fann_D8_5m.tif')\n",
    "D8 = np.array(input_geotiff.GetRasterBand(1).ReadAsArray())\n",
    "\n",
    "input_geotiff = gdal.Open('fann_Dinf_5m.tif')\n",
    "Dinf = np.array(input_geotiff.GetRasterBand(1).ReadAsArray())\n",
    "\n",
    "input_geotiff = gdal.Open('fann_M4_5m.tif')\n",
    "M4 = np.array(input_geotiff.GetRasterBand(1).ReadAsArray())\n",
    "\n",
    "input_geotiff = gdal.Open('fann_M8_5m.tif')\n",
    "M8 = np.array(input_geotiff.GetRasterBand(1).ReadAsArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D4_dict = catchmentDicts(D4, grid_nodes, catchment_area_node_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D8_dict = catchmentDicts(D8, grid_nodes, catchment_area_node_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dinf_dict = catchmentDicts(Dinf, grid_nodes, catchment_area_node_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M4_dict = catchmentDicts(M4, grid_nodes, catchment_area_node_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M8_dict = catchmentDicts(M8, grid_nodes, catchment_area_node_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = [D4_dict, D8_dict, Dinf_dict, M4_dict, M8_dict]\n",
    "d = {}\n",
    "for k in D4_dict.keys():\n",
    "    d[k] = tuple(d[k] for d in ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catchment_areas = pd.DataFrame.from_dict(d, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catchment_areas.rename(columns={0: 'D4', 1: 'D8', 2: 'Dinf', 3: 'M4', 4: 'M8'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catchment_areas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_id_column = np.array(catchment_areas.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catchment_areas['node_id'] = node_id_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catchment_areas.to_csv(r'C:/PhD/junk/catchment_areas_5m.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_5m_divergent = catchment_areas_5m_divergent.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catchment_areas_1m_convergent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catchment_areas_1m_divergent['Shape'] = 'Divergent'\n",
    "catchment_areas_1m_convergent['Shape'] = 'Convergent'\n",
    "catchment_areas_2m_divergent['Shape'] = 'Divergent'\n",
    "catchment_areas_2m_convergent['Shape'] = 'Convergent'\n",
    "catchment_areas_3m_divergent['Shape'] = 'Divergent'\n",
    "catchment_areas_3m_convergent['Shape'] = 'Convergent'\n",
    "catchment_areas_4m_divergent['Shape'] = 'Divergent'\n",
    "catchment_areas_4m_convergent['Shape'] = 'Convergent'\n",
    "catchment_areas_5m_divergent['Shape'] = 'Divergent'\n",
    "catchment_areas_5m_convergent['Shape'] = 'Convergent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_data = pd.concat([catchment_areas_1m_divergent,catchment_areas_2m_convergent,\n",
    "                      catchment_areas_2m_divergent,catchment_areas_2m_convergent,\n",
    "                      catchment_areas_3m_divergent,catchment_areas_3m_convergent,\n",
    "                      catchment_areas_4m_divergent,catchment_areas_4m_convergent,\n",
    "                      catchment_areas_5m_divergent,catchment_areas_5m_convergent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = area_data[['D4', 'Shape']]\n",
    "b = area_data[['D8', 'Shape']]\n",
    "c = area_data[['Dinf', 'Shape']]\n",
    "d = area_data[['M4', 'Shape']]\n",
    "e = area_data[['M8', 'Shape']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a['Method'] = 'D4'\n",
    "b['Method'] = 'D8'\n",
    "c['Method'] = 'Dinf'\n",
    "d['Method'] = 'M4'\n",
    "e['Method'] = 'M8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = a.rename(columns={'D4':'Area'})\n",
    "B = b.rename(columns={'D8':'Area'})\n",
    "C = c.rename(columns={'Dinf':'Area'})\n",
    "D = d.rename(columns={'M4':'Area'})\n",
    "E = e.rename(columns={'M8':'Area'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_data = pd.concat([A,B,C,D,E])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_data_reformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot = sns.distplot( A[\"Area\"] , color=\"skyblue\")\n",
    "plot = sns.distplot( B[\"Area\"] , color=\"olive\")\n",
    "plot = sns.distplot( C[\"Area\"] , color=\"gold\")\n",
    "plot = sns.distplot( D[\"Area\"] , color=\"teal\")\n",
    "plot.set_xlim(-1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot miles per gallon against horsepower with other semantics\n",
    "sns.set(font_scale = 1.2)\n",
    "sns.set_style(\"whitegrid\", {'grid.linestyle': '--'})\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 5))\n",
    "g = sns.boxplot(x=\"Method\", y=\"Area\", palette=\"vlag\", hue = 'Shape', data=area_data)\n",
    "\n",
    "ax.set_ylabel('Catchment area (ha)', fontsize = 15)\n",
    "ax.set_xlabel('Routing algorithm', fontsize = 15)\n",
    "ax.set_ylim(-0.1, 4)\n",
    "#plt.title('Partitioning behaviour between adjacent gully heads', fontsize = 20)\n",
    "\n",
    "hatches = [\"\", \"/\", \"\",  \"/\", \"\",  \"/\", \"\",  \"/\", \"\", \"/\"]\n",
    "for hatch, patch in zip(hatches, g.artists):\n",
    "    patch.set_hatch(hatch)\n",
    "    \n",
    "hatches = [\"\", \"/\"]\n",
    "\n",
    "# Loop over the bars\n",
    "for i,thisbar in enumerate(g.patches):\n",
    "    # Set a different hatch for each bar\n",
    "    thisbar.set_hatch(hatches[i])\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., title = 'Flow above gully',fontsize='15')\n",
    "plt.setp(ax.get_legend().get_title(), fontsize='15') # for legend title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set(style = \"ticks\")\n",
    "colors = [\"xkcd:very dark blue\", \"xkcd:mauve\"]\n",
    "\n",
    "def plot_unity(xdata, ydata, **kwargs):\n",
    "    mn = min(xdata.min(), ydata.min())\n",
    "    mx = max(xdata.max(), ydata.max())\n",
    "    points = np.linspace(mn, mx, 100)\n",
    "    plt.gca().plot(points, points, color = 'xkcd:merlot', marker=None,\n",
    "            linestyle='--', linewidth=1.0)\n",
    "\n",
    "g = sns.pairplot(area_data, palette = colors, hue = 'Shape', markers=['x', 'o'])\n",
    "\n",
    "sns.plotting_context()\n",
    "g.map_offdiag(plot_unity)\n",
    "\n",
    "g.set(ylim=(-0.5,6))\n",
    "g.set(xlim=(-0.5,6))\n",
    "plt.rcParams[\"axes.labelsize\"] = 15\n",
    "for ax in plt.gcf().axes:\n",
    "    l = ax.get_xlabel()\n",
    "    m = ax.get_ylabel()\n",
    "    ax.set_xlabel(l, fontsize=15)\n",
    "    ax.set_ylabel(m, fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (8, 8))\n",
    "data = area_data\n",
    "g0 = sns.scatterplot(x=\"M8\", y=\"Dinf\", marker = 'x', edgecolor= 'xkcd:royal blue', legend = False, \n",
    "                     linewidth = 1,data=data, hue = 'Shape')\n",
    "g1 = sns.scatterplot(x=\"M8\", y=\"D8\", alpha=0.4, color='xkcd:merlot', edgecolor= 'xkcd:merlot', \n",
    "                      linewidth = 1, data=data, hue = 'Shape')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., title = '',fontsize='15')\n",
    "plt.setp(ax.get_legend().get_title(), fontsize='15') # for legend title\n",
    "plt.plot([0, 10], [0, 10], linestyle='--', color = 'xkcd:apricot')\n",
    "ax.set_ylim(-0.1,2)\n",
    "ax.set_xlim(-0.1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('ticks', {'grid.linestyle': '--'})\n",
    "\n",
    "colours = {'Convergent':'xkcd:mid blue', 'Divergent':'xkcd:pale pink'}\n",
    "x_axis = 'M8'\n",
    "y_axis = 'M4'\n",
    "g = sns.relplot(x=x_axis, y=y_axis,  palette = colours, hue = 'Shape',\n",
    "                 data=area_data, alpha = 0.7, legend = 'brief', edgecolor=\"k\", style = 'Shape')\n",
    "\n",
    "ax = g.axes[0,0]\n",
    "ax.set_xlim(-0.1,6)\n",
    "ax.set_ylim(-0.1,6)\n",
    "ax.set_xlabel(x_axis, fontsize = 20)\n",
    "ax.set_ylabel(y_axis, fontsize = 20)\n",
    "leg = g._legend\n",
    "plt.rcParams[\"axes.edgecolor\"] = \"0.15\"\n",
    "plt.plot([0, 10], [0, 10], linestyle='--', color = 'xkcd:chocolate', alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_5m_divergent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style = 'ticks')\n",
    "\n",
    "data = corr_5m_divergent\n",
    "\n",
    "mask = np.triu(np.ones_like(data, dtype=np.bool))\n",
    "\n",
    "comparison = sns.heatmap(data, xticklabels=data.columns, mask=mask,\n",
    "                         yticklabels=data.columns, annot = True,\n",
    "            cmap=sns.diverging_palette(220, 10, as_cmap=True), vmin = 0.5, vmax = 1, linecolor = 'white', linewidths = 3)\n",
    "\n",
    "comparison.set_xticklabels(comparison.get_xticklabels(), rotation=45)\n",
    "comparison.set_yticklabels(comparison.get_yticklabels(), rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_dict_convergent = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation_dict['D4/D8'] = [c.iloc[1,0], c_2m.iloc[1,0], c_3m.iloc[1,0], c_4m.iloc[1,0], c_5m.iloc[1,0]]\n",
    "#correlation_dict['D4/Dinf'] = [c.iloc[2,0], c_2m.iloc[2,0], c_3m.iloc[2,0], c_4m.iloc[2,0], c_5m.iloc[2,0]]\n",
    "#correlation_dict['D4/M4'] = [c.iloc[3,0], c_2m.iloc[3,0], c_3m.iloc[3,0], c_4m.iloc[3,0], c_5m.iloc[3,0]]\n",
    "correlation_dict_convergent['D4/M8'] = [corr_1m_convergent.iloc[4,0], \n",
    "                             corr_2m_convergent.iloc[4,0], corr_3m_convergent.iloc[4,0], \n",
    "                             corr_4m_convergent.iloc[4,0], corr_5m_convergent.iloc[4,0]]\n",
    "#correlation_dict['D8/Dinf'] = [c.iloc[2,1], c_2m.iloc[2,1], c_3m.iloc[2,1], c_4m.iloc[2,1], c_5m.iloc[2,1]]\n",
    "#correlation_dict['D8/M4'] = [c.iloc[3,1], c_2m.iloc[3,1], c_3m.iloc[3,1], c_4m.iloc[3,1], c_5m.iloc[3,1]]\n",
    "correlation_dict_convergent['D8/M8'] = [corr_1m_convergent.iloc[4,1], \n",
    "                             corr_2m_convergent.iloc[4,1], corr_3m_convergent.iloc[4,1], \n",
    "                             corr_4m_convergent.iloc[4,1], corr_5m_convergent.iloc[4,1]]\n",
    "#correlation_dict['Dinf/M4'] = [c.iloc[3,2], c_2m.iloc[3,2], c_3m.iloc[3,2], c_4m.iloc[3,2], c_5m.iloc[3,2]]\n",
    "correlation_dict_convergent['Dinf/M8'] = [corr_1m_convergent.iloc[4,2], \n",
    "                               corr_2m_convergent.iloc[4,2], corr_3m_convergent.iloc[4,2], \n",
    "                               corr_4m_convergent.iloc[4,2], corr_5m_convergent.iloc[4,2]]\n",
    "correlation_dict_convergent['M4/M8'] = [corr_1m_convergent.iloc[4,3], \n",
    "                             corr_2m_convergent.iloc[4,3], corr_3m_convergent.iloc[4,3], \n",
    "                             corr_4m_convergent.iloc[4,3], corr_5m_convergent.iloc[4,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " correlation_convergent_dataframe = pd.DataFrame.from_dict(correlation_dict_convergent, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergent_corr_1m =  correlation_convergent_dataframe.iloc[:,0].to_frame()\n",
    "convergent_corr_2m =  correlation_convergent_dataframe.iloc[:,1].to_frame()\n",
    "convergent_corr_3m =  correlation_convergent_dataframe.iloc[:,2].to_frame()\n",
    "convergent_corr_4m =  correlation_convergent_dataframe.iloc[:,3].to_frame()\n",
    "convergent_corr_5m =  correlation_convergent_dataframe.iloc[:,4].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergent_corr_1m_rename = convergent_corr_1m.rename(columns={0: \"Correlation\"})\n",
    "convergent_corr_2m_rename = convergent_corr_2m.rename(columns={1: \"Correlation\"})\n",
    "convergent_corr_3m_rename = convergent_corr_3m.rename(columns={2: \"Correlation\"})\n",
    "convergent_corr_4m_rename = convergent_corr_4m.rename(columns={3: \"Correlation\"})\n",
    "convergent_corr_5m_rename = convergent_corr_5m.rename(columns={4: \"Correlation\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergent_corr_1m_rename.insert(1, 'Resolution', True) \n",
    "convergent_corr_2m_rename.insert(1, 'Resolution', True) \n",
    "convergent_corr_3m_rename.insert(1, 'Resolution', True) \n",
    "convergent_corr_4m_rename.insert(1, 'Resolution', True) \n",
    "convergent_corr_5m_rename.insert(1, 'Resolution', True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergent_corr_1m_rename.insert(2, \"Method\", ['D4/M8', 'D8/M8', 'Dinf/M8','M4/M8'], True)  \n",
    "convergent_corr_2m_rename.insert(2, \"Method\", ['D4/M8', 'D8/M8', 'Dinf/M8','M4/M8'], True)  \n",
    "convergent_corr_3m_rename.insert(2, \"Method\", ['D4/M8', 'D8/M8', 'Dinf/M8','M4/M8'], True)  \n",
    "convergent_corr_4m_rename.insert(2, \"Method\", ['D4/M8', 'D8/M8', 'Dinf/M8','M4/M8'], True) \n",
    "convergent_corr_5m_rename.insert(2, \"Method\", ['D4/M8', 'D8/M8', 'Dinf/M8','M4/M8'], True)\n",
    "\n",
    "#corr_1m_rename.insert(2, \"Method\", ['D4/D8','D4/Dinf', 'D4/M4', \n",
    "#                                           'D4/M8', 'D8/Dinf', 'D8/M4', 'D8/M8', 'Dinf/M4', 'Dinf/M8','M4/M8'], True) \n",
    "#corr_2m_rename.insert(2, \"Method\", ['D4/D8','D4/Dinf', 'D4/M4', \n",
    "#                                           'D4/M8', 'D8/Dinf', 'D8/M4', 'D8/M8', 'Dinf/M4', 'Dinf/M8','M4/M8'], True) \n",
    "#corr_3m_rename.insert(2, \"Method\", ['D4/D8','D4/Dinf', 'D4/M4', \n",
    "#                                           'D4/M8', 'D8/Dinf', 'D8/M4', 'D8/M8', 'Dinf/M4', 'Dinf/M8','M4/M8'], True) \n",
    "#corr_4m_rename.insert(2, \"Method\", ['D4/D8','D4/Dinf', 'D4/M4', \n",
    "#                                           'D4/M8', 'D8/Dinf', 'D8/M4', 'D8/M8', 'Dinf/M4', 'Dinf/M8','M4/M8'], True) \n",
    "#corr_5m_rename.insert(2, \"Method\", ['D4/D8','D4/Dinf', 'D4/M4', \n",
    "#                                           'D4/M8', 'D8/Dinf', 'D8/M4', 'D8/M8', 'Dinf/M4', 'Dinf/M8','M4/M8'], True) \n",
    "\n",
    "\n",
    "convergent_corr_1m_rename['Resolution'] = '1m'\n",
    "convergent_corr_2m_rename['Resolution'] = '2m'\n",
    "convergent_corr_3m_rename['Resolution'] = '3m'\n",
    "convergent_corr_4m_rename['Resolution'] = '4m'\n",
    "convergent_corr_5m_rename['Resolution'] = '5m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergent_combined_correlations = convergent_corr_1m_rename.append([convergent_corr_2m_rename, \n",
    "                                                                   convergent_corr_3m_rename,\n",
    "                                                                   convergent_corr_4m_rename,\n",
    "                                                                   convergent_corr_5m_rename])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergent_combined_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style = 'ticks')\n",
    "fig, ax = plt.subplots(figsize = (10, 5))\n",
    "data = convergent_combined_correlations\n",
    "colours = {'D4/M8':'xkcd:burnt yellow', 'D8/M8':'xkcd:merlot', 'Dinf/M8':'xkcd:dark sea green', 'M4/M8':'xkcd:cobalt blue'}\n",
    "g = sns.stripplot(x = 'Resolution', y = 'Correlation', hue = 'Method',size=15, marker=\"o\",\n",
    "                   edgecolor=\"black\", alpha=.7, palette = colours,\n",
    "             data=data,  linewidth=1)\n",
    "\n",
    "plt.grid(linestyle='--')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "g.set(ylim = (0.6, 1))\n",
    "#for ind, label in enumerate(g.get_xticklabels()):\n",
    "#    if ind % 2 == 0:  # every 10th label is kept\n",
    "#        label.set_visible(False)\n",
    "#    else:\n",
    "#        label.set_visible(True)\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "ax.set_xlabel('Resolution', fontsize = 20)\n",
    "ax.set_ylabel('Correlation', fontsize = 20)\n",
    "plt.setp(ax.get_legend().get_texts(), fontsize='15') # for legend text\n",
    "plt.setp(ax.get_legend().get_title(), fontsize='22') # for legend title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style = 'ticks')\n",
    "fig, ax = plt.subplots(figsize = (10, 6))\n",
    "jitter = 0.15\n",
    "data = convergent_combined_correlations\n",
    "ylim_min = .6\n",
    "ylim_max = 1\n",
    "#plt.clf()\n",
    "D4p = data[(data['Method']=='D4/M8')]\n",
    "colors = ['burnt yellow']\n",
    "m = sns.stripplot('Resolution','Correlation',hue='Method',\n",
    "                  marker='o',data=D4p, jitter=jitter, \n",
    "                  palette=sns.xkcd_palette(colors),\n",
    "                  split=True,linewidth=2,edgecolor=\"black\", size = 15, alpha = 0.7)\n",
    "m.set(ylim = (ylim_min, ylim_max))\n",
    "D8p = data[(data['Method']=='D8/M8')]\n",
    "colors = ['merlot']\n",
    "n = sns.stripplot('Resolution','Correlation',hue='Method',\n",
    "                  marker='v',data=D8p, jitter=jitter, \n",
    "                  palette=sns.xkcd_palette(colors),\n",
    "                  split=True,linewidth=2,edgecolor=\"black\", size = 15, alpha = 0.7)\n",
    "\n",
    "Dinfp = data[(data['Method']=='Dinf/M8')]\n",
    "colors = ['dark sea green']\n",
    "o = sns.stripplot('Resolution','Correlation',hue='Method',\n",
    "                  marker='s',data=Dinfp, jitter=jitter, \n",
    "                  palette=sns.xkcd_palette(colors),\n",
    "                  split=True,linewidth=2,edgecolor=\"black\", size = 15, alpha = 0.7)\n",
    "\n",
    "M4p = data[(data['Method']=='M4/M8')]\n",
    "colors = ['cobalt blue']\n",
    "p = sns.stripplot('Resolution','Correlation',hue='Method',\n",
    "                  marker='D',data=M4p, jitter=jitter, \n",
    "                  palette=sns.xkcd_palette(colors),\n",
    "                  split=True,linewidth=2,edgecolor=\"black\", size = 15, alpha = 0.7)\n",
    "\n",
    "plt.grid(linestyle='--')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "g.set(ylim = (ylim_min, ylim_max))\n",
    "ax.set_xlabel('Resolution', fontsize = 15)\n",
    "ax.set_ylabel('Correlation', fontsize = 15)\n",
    "plt.setp(ax.get_legend().get_texts(), fontsize='15') # for legend text\n",
    "plt.setp(ax.get_legend().get_title(), fontsize='22') # for legend title\n",
    "\n",
    "D4l = Line2D([], [], color='xkcd:burnt yellow', marker='o', linestyle='None',\n",
    "                          markersize=15, label='D4-M8')\n",
    "D8l = Line2D([], [], color='xkcd:merlot', marker='v', linestyle='None',\n",
    "                          markersize=15, label='D8-M8')\n",
    "Dinfl = Line2D([], [], color='xkcd:dark sea green', marker='s', linestyle='None',\n",
    "                          markersize=15, label='Dinf-M8')\n",
    "M4l = Line2D([], [], color='xkcd:cobalt blue', marker='D', linestyle='None',\n",
    "                          markersize=15, label='M4-M8')\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "ax.set_xlabel('Resolution', fontsize = 20)\n",
    "ax.set_ylabel('Correlation', fontsize = 20)\n",
    "\n",
    "plt.legend(handles=[D4l, D8l, Dinfl,M4l],fontsize=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = c_5m\n",
    "\n",
    "mask = np.triu(np.ones_like(data, dtype=np.bool))\n",
    "\n",
    "comparison = sns.heatmap(data, xticklabels=c.columns, mask=mask,\n",
    "                         yticklabels=c.columns, annot = True,\n",
    "            cmap=sns.diverging_palette(220, 10, as_cmap=True), vmin = 0.8, vmax = 1, linecolor = 'white', linewidths = 3)\n",
    "\n",
    "comparison.set_xticklabels(comparison.get_xticklabels(), rotation=45)\n",
    "comparison.set_yticklabels(comparison.get_yticklabels(), rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "isolate_catchment_area_nodes = np.isin(grid_nodes, catchment_area_node_list).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(isolate_catchment_area_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(isolate_catchment_area_nodes, cmap=\"terrain\");\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def np_array_to_Geotiff(newfile, original_tiff, np_array, dtype):\n",
    "    \n",
    "    cols = np_array.shape[1]\n",
    "    rows = np_array.shape[0]\n",
    "    originX, pixelWidth, b, originY, d, pixelHeight = original_tiff.GetGeoTransform() \n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    GDT_dtype = gdal.GDT_Unknown\n",
    "    if dtype == \"Float64\": \n",
    "        GDT_dtype = gdal.GDT_Float64\n",
    "    elif dtype == \"Float32\":\n",
    "        GDT_dtype = gdal.GDT_Float32\n",
    "    else:\n",
    "        print(\"Not supported data type.\")\n",
    "    \n",
    "    if np_array.ndim == 2:\n",
    "        band_num = 1\n",
    "    else:\n",
    "        band_num = np_array.shape[2]\n",
    "\n",
    "    outRaster = driver.Create(newfile, cols, rows, band_num, GDT_dtype)\n",
    "    outRaster.SetGeoTransform((originX, pixelWidth, 0, originY, 0, pixelHeight))\n",
    "    \n",
    "    # Loop over all bands.\n",
    "    for b in range(band_num):\n",
    "        outband = outRaster.GetRasterBand(b + 1)\n",
    "    \n",
    "        # Read in the band's data into the third dimension of our array\n",
    "        if band_num == 1:\n",
    "            outband.WriteArray(np_array)\n",
    "        else:\n",
    "            outband.WriteArray(np_array[:,:,b])\n",
    "\n",
    "    # setteing srs from input tif file.\n",
    "    prj=original_tiff.GetProjection()\n",
    "    outRasterSRS = osr.SpatialReference(wkt=prj)\n",
    "    outRaster.SetProjection(outRasterSRS.ExportToWkt())\n",
    "    outband.FlushCache()\n",
    "    outRaster = None\n",
    "    \n",
    "    return outRaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array_to_Geotiff('outlet_t2.tif', input_geotiff, outlet_ids, input_DEM.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
